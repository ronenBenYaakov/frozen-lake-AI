{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p878407CdKxq"
      },
      "outputs": [],
      "source": [
        "!pip install gym"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import gym\n",
        "import random\n",
        "import time\n",
        "from IPython.display import clear_output"
      ],
      "metadata": {
        "id": "sPZ3DISfdNOr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make('FrozenLake-v1')"
      ],
      "metadata": {
        "id": "llqZNJqlfEnh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "action_space_size = env.action_space.n\n",
        "state_space_size = env.observation_space.n\n",
        "Q = np.zeros((state_space_size, action_space_size))"
      ],
      "metadata": {
        "id": "4F9kNpTVfTO3",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_episodes = 10 ** 4\n",
        "max_steps_per_episode = 100\n",
        "alpha, gamma, epsilon, max_epsilon, min_epsilon, csi = 0.1, 0.99, 1, 1, 0.1, 0.01"
      ],
      "metadata": {
        "id": "OZP6gVpmgcZu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define parameters for the linear equation\n",
        "csi = 0.01  # ξ (slope)\n",
        "epsilon = 1  # ɛ (intercept)\n",
        "\n",
        "# Data for the linear graph\n",
        "x = np.linspace(0, 99, 100)  # Generate 100 points between 0 and 90\n",
        "y = -1 * csi * x + epsilon  # Linear equation: y = -ξx + ɛ\n",
        "\n",
        "# Plotting the linear graph\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(x, y, label=r'$y = -\\xi x + \\epsilon$', color='blue', linewidth=2)\n",
        "\n",
        "# Adding a horizontal line at y = 0.01\n",
        "plt.axhline(y=0.01, color='red', linestyle='--', label='y = 0.01')\n",
        "\n",
        "# Adding labels and title\n",
        "plt.title(\"Exploration Rate: y = -ξx + ɛ\", fontsize=16)\n",
        "plt.xlabel(\"Current Step\", fontsize=14)\n",
        "plt.ylabel(\"Exploration Rate\", fontsize=14)\n",
        "\n",
        "# Adding grid and legend\n",
        "plt.grid(True, linestyle='--', alpha=0.7)\n",
        "plt.axhline(0, color='black', linewidth=0.8)  # X-axis\n",
        "plt.axvline(0, color='black', linewidth=0.8)  # Y-axis\n",
        "plt.legend(fontsize=12)\n",
        "\n",
        "# Show the plot\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "cIMsJSegj8kG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rewards_all_episodes = []\n",
        "\n",
        "# Q-learning loop\n",
        "for episode in range(num_episodes):\n",
        "    s = env.reset()  # Reset the environment\n",
        "    done = False\n",
        "    current_episode_reward = 0\n",
        "\n",
        "    for step in range(max_steps_per_episode):\n",
        "        # Choose action using epsilon-greedy policy\n",
        "        r = random.uniform(0, 1)\n",
        "        if r > epsilon:\n",
        "            a = np.argmax(Q[s, :])  # Exploit: Choose the best action\n",
        "        else:\n",
        "            a = env.action_space.sample()  # Explore: Random action\n",
        "\n",
        "        # Take action and observe reward and next state\n",
        "        new_state, reward, done, info = env.step(a)\n",
        "\n",
        "        # Update Q(s, a) using the Q-learning update rule\n",
        "        Q[s, a] = (1 - alpha) * Q[s, a] + alpha * (reward + gamma * np.max(Q[new_state, :]))\n",
        "\n",
        "        # Update state and accumulate reward\n",
        "        s = new_state\n",
        "        current_episode_reward += reward\n",
        "\n",
        "        if done:  # End episode if done\n",
        "            break\n",
        "\n",
        "    # Update exploration rate after the episode\n",
        "    epsilon = min_epsilon + (max_epsilon - min_epsilon) * np.exp(-csi * episode)\n",
        "\n",
        "    # Store total reward for this episode\n",
        "    rewards_all_episodes.append(current_episode_reward)\n"
      ],
      "metadata": {
        "id": "UN_MEnvsm6Kp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(rewards_all_episodes)\n"
      ],
      "metadata": {
        "id": "XxspcfMzqx97"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Q"
      ],
      "metadata": {
        "id": "jP_iuzfawpBX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "huXgHsfZwtqA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}